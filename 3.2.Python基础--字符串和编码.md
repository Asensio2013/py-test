### 字符编码

* ASCII 编码
    * 最早只有127个字符被编码到计算机里, 也就是大小写英文字母、数字和一些符号


* GB2312 编码
    * 处理中文显然一个字节是不够的, 至少需要两个字节, 而且还不能和ASCII编码冲突, 所以, 中国制定了 GB2312 编码, 用来把中文编进去

    * 于是各种语言各种编码, 结果就出现了乱码


* Unicode 编码
    * 为了解决乱码, Unicode应运而生.Unicode把所有语言都统一到一套编码里

    * Unicode标准也在不断发展, 但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符, 就需要4个字节）.现代操作系统和大多数编程语言都直接支持Unicode.

    * ASCII编码和Unicode编码的区别：ASCII编码是1个字节, 而Unicode编码通常是2个字节

    * 新的问题又出现了：如果统一成Unicode编码, 乱码问题从此消失了.但是, 如果你写的文本基本上全部是英文的话, 用Unicode编码比ASCII编码需要多一倍的存储空间, 在存储和传输上就十分不划算


* UTF-8 编码
    * 所以, 本着节约的精神, 又出现了把 Unicode 编码转化为“可变长编码”的 UTF-8 编码

    * UTF-8 编码把一个 Unicode 字符根据不同的数字大小编码成1-6个字节, 常用的英文字母被编码成1个字节, 汉字通常是3个字节, 只有很生僻的字符才会被编码成4-6个字节

    * UTF-8 编码有一个额外的好处, 就是 ASCII 编码实际上可以被看成是 UTF-8 编码的一部分, 所以, 大量只支持 ASCII 编码的历史遗留软件可以在 UTF-8 编码下继续工作


* ASCII、Unicode 和 UTF-8 的关系:
    * 总结一下现在计算机系统通用的字符编码工作方式

    * 在计算机 *内存中* , 统一使用 Unicode 编码, 当需要保存到硬盘或者需要传输的时候, 就转换为 UTF-8 编码.

    * eg: 用记事本编辑的时候, 从文件读取的 UTF-8 字符被转换为 Unicode 字符到内存里, 编辑完成后, 保存的时候再把 Unicode 转换为 UTF-8 保存到文件



### Python的字符串

* 对于单个字符的编码，Python 提供了
    * `ord()` 函数获取字符的整数表示

    * `chr()` 函数把编码转换为对应的字符

    * 十六进制写 str: `'\u4e2d\u6587'`

    * 把 `str` 变为以 *字节* 为单位的 `bytes`: `b'ABC'`
        * `bytes` 应用场景 在网络上传输，或者保存到磁盘上

    * `encode()` 方法, 指定 `str` 的编码:
        * `'ABC'.encode('ascii')` # b'ABC'

        * `'中文'.encode('utf-8')` # b'\xe4\xb8\xad\xe6\x96\x87'

    * `decode()` 方法, 指定 `str` 的解码:
        * `b'ABC'.decode('ascii')` # 'ABC'

        * `b'\xe4\xb8\xad\xe6\x96\x87'.decode('utf-8')` # '中文'

        * 应用场景 从网络或磁盘上读取了字节流，那么读到的数据就是 `bytes`, 就需要解码了

    * `len()` 函数, 计算 `str` 包含多少个字符:
        * `len('ABC')` # 3

        * `len('中文')` # 2


* 在操作字符串时，我们经常遇到 `str` 和 `bytes`的互相转换
    * 为了避免乱码问题，应当始终坚持使用 UTF-8 编码对 `str` 和 `bytes` 进行转换


* py 文件指定编码:
    * 文件开头加上 `# -*- coding: utf-8 -*-`

    * 当然在此之前通常还会加上 `#!/usr/bin/env python3`


* 格式化
    * 占位符 `%d`, `%f`, `%s`, `%x`

    * 如果不太确定应该用什么，`%s` 永远起作用，它会把任何数据类型转换为字符串
